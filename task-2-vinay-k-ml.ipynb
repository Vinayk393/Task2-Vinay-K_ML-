{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":74935,"sourceType":"datasetVersion","datasetId":42674}],"dockerImageVersionId":29962,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data <a id =\"12\"></a>\n* We are going to use ‘Mall_Customers.csv’ CSV file\n* Dataset contains 5 columns CustomerID, Gender, Age, Annual Income (k$), Spending Score (1-100)","metadata":{}},{"cell_type":"code","source":"#df = pd.read_csv('https://raw.githubusercontent.com/satishgunjal/datasets/master/Mall_Customers.csv')\ndf = pd.read_csv('/kaggle/input/customer-segmentation-tutorial-in-python/Mall_Customers.csv')\nprint(\"Shape of the data= \", df.shape)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Understanding The Data <a id =\"13\"></a>\n* There are total 200 training example without any label to indicate which customer belong which group\n* We are going to use annual income and spending score to find the clusters in data. Note that spending score is from 1 to 100 which is assigned by the mall based on customer behavior and spending nature ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.scatter(df['Annual Income (k$)'],df['Spending Score (1-100)'])\nplt.xlabel('Annual Income')\nplt.ylabel('Spending Score')\nplt.title('Unlabelled Mall Customer Data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Since we are going to use Annual Income and Spending Score  columns only, lets create 2D array of these columns for further use\nX = df.iloc[:, [3,4]].values\nX[:5] # Show first 5 records only","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Choosing The Number of Clusters <a id =\"14\"></a>\nBy visual inspection of above scatter plot, we can identify 5 possible clusters. But since there is no other information available its very difficult say it with 100% confidence. So lets try to verify this with Elbow method technique.\n\n### Elbow Method <a id =\"15\"></a>\n* Using the elbow method to find the optimal number of clusters. Let's use 1 to 11 as range of clusters.\n* We will use 'random' initialization method for this study.\n* Note that Sklearn K-Means algorithm also have ‘k-means++’ initialization method. It selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.","metadata":{}},{"cell_type":"code","source":"clustering_score = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'random', random_state = 42)\n    kmeans.fit(X)\n    clustering_score.append(kmeans.inertia_) # inertia_ = Sum of squared distances of samples to their closest cluster center.\n    \n\nplt.figure(figsize=(10,6))\nplt.plot(range(1, 11), clustering_score)\nplt.scatter(5,clustering_score[4], s = 200, c = 'red', marker='*')\nplt.title('The Elbow Method')\nplt.xlabel('No. of Clusters')\nplt.ylabel('Clustering Score')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"From above elbow plot its clear that clustering scores slows down after 5 number of clusters. So we can use K= 5 for further analysis.\n\n## Compute K-Means Clustering <a id =\"16\"></a>\nCompute cluster centers and predict cluster index for each sample. Since K=5 we will get the cluster index from 0 to 4 for every data point in our dataset.","metadata":{}},{"cell_type":"code","source":"kmeans= KMeans(n_clusters = 5, random_state = 42)\n\n# Compute k-means clustering\nkmeans.fit(X)\n\n# Compute cluster centers and predict cluster index for each sample.\npred = kmeans.predict(X)\n\npred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'pred' contains the values index( 0 to 4) cluster for every training example. Let's add it to original dataset for better understanding.","metadata":{}},{"cell_type":"code","source":"df['Cluster'] = pd.DataFrame(pred, columns=['cluster'] )\nprint('Number of data points in each cluster= \\n', df['Cluster'].value_counts())\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization <a id =\"17\"></a>\nLet's plot the centroid and cluster with different colors to visualize, how K-Means algorithm has grouped the data.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.scatter(X[pred == 0, 0], X[pred == 0, 1], c = 'brown', label = 'Cluster 0')\nplt.scatter(X[pred == 1, 0], X[pred == 1, 1], c = 'green', label = 'Cluster 1')\nplt.scatter(X[pred == 2, 0], X[pred == 2, 1], c = 'blue', label = 'Cluster 2')\nplt.scatter(X[pred == 3, 0], X[pred == 3, 1], c = 'purple', label = 'Cluster 3')\nplt.scatter(X[pred == 4, 0], X[pred == 4, 1], c = 'orange', label = 'Cluster 4')\n\nplt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:, 1],s = 300, c = 'red', label = 'Centroid', marker='*')\n\nplt.xlabel('Annual Income')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.title('Customer Clusters')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inner Working <a id =\"18\"></a>\nUsing below code we can visualize the inner working of K-Means algorithm.\n\n* To start with we will define the random centroids. You can see in below plot that initial centroids with original data without any clusters.\n* In step 1 we will run the K-Means algorithm only for one iteration and plot the new position of centroid. Notice how centroid position changes and clusters started to form around it.\n* In step 2, we will run the K-Means algorithm for two iterations. Notice how data points are reassigned from one cluster to another as centroid position change\n* Similarly at the end we run the K-Means algorithm for six iterations, where we get the final location of centroids and associated clusters.","metadata":{}},{"cell_type":"code","source":"def plot_k_means_progress(centroid_history,n_clusters, centroid_sets, cluster_color):\n    \"\"\"\n    This function will plot the path taken by the centroids\n    \n    I/P:\n    * centroid_history: 2D array of centroids. Each element represent the centroid coordinate. \n      If there are 5 clusters then first set contains initial cluster cordinates\n      (i.e. first 5 elements) and then k_means loop will keep appending new cluster coordinates for each iteration\n    * n_clusters: Total number of clusters to find\n    * centroid_sets: At the start we set random values as our first centroid set. K-Means loop will keep adding \n    new centroid sets to centroid_history. Since we are ploting the path of centroid locations, centroid set value \n    will be K-Means loop iteration number plus 1 for initial centroid set. \n    So its value will be from 2 to K-Means loops max iter plus 1\n    * cluster_color: Just to have same line and cluster color\n    \n    O/P: Plot the centroid path\n    \"\"\"\n    c_x = [] # To store centroid X coordinated\n    c_y=[]   # To store the centroid Y coordinates\n    for i in range(0, n_clusters):\n        cluster_index = 0\n        for j in range(0, centroid_sets):\n            c_x = np.append(c_x, centroid_history[:,0][i + cluster_index])\n            c_y = np.append(c_y, centroid_history[:,1][i + cluster_index])\n            cluster_index = cluster_index + n_clusters\n            # if there are 5 clusters then first set contains initial cluster cordinates and then k_means loop will keep appending new cluster coordinates for each iteration\n        \n        plt.plot(c_x, c_y, c= cluster_color['c_' + str(i)], linestyle='--')\n        \n        # Reset coordinate arrays to avoid continuous lines\n        c_x = []\n        c_y=[]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\n\n# Random Initialization of Centroids\nplt.scatter(df['Annual Income (k$)'],df['Spending Score (1-100)'])\ninitial_centroid = np.array([[10, 2], [50,100], [130,20], [50,15], [140,100]])\n\nplt.scatter(initial_centroid[:,0], initial_centroid[:, 1],s = 200, c = 'red', label = 'Random Centroid', marker='*')\nplt.xlabel('Annual Income')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.title('Random Initialization of Centroids')\n\n# K-Means loop of assignment and move centroid steps\ncentroid_history = []\ncentroid_history = initial_centroid\n#\ncluster_color= {'c_0':'brown','c_1':'green','c_2':'blue','c_3':'purple','c_4':'orange'}\nn_clusters = 5\nfor i in range(1,6):\n    kmeans= KMeans(n_clusters, init= initial_centroid, n_init= 1, max_iter= i, random_state = 42)  #n_init= 1 since our init parameter is array\n    \n    # Compute cluster centers and predict cluster index for each sample\n    pred = kmeans.fit_predict(X)\n\n    plt.figure(figsize=(10,6))\n    plt.scatter(X[pred == 0, 0], X[pred == 0, 1], c = 'brown', label = 'Cluster 0')\n    plt.scatter(X[pred == 1, 0], X[pred == 1, 1], c = 'green', label = 'Cluster 1')\n    plt.scatter(X[pred == 2, 0], X[pred == 2, 1], c = 'blue', label = 'Cluster 2')\n    plt.scatter(X[pred == 3, 0], X[pred == 3, 1], c = 'purple', label = 'Cluster 3')\n    plt.scatter(X[pred == 4, 0], X[pred == 4, 1], c = 'orange', label = 'Cluster 4') \n    \n    plt.scatter(centroid_history[:,0], centroid_history[:, 1],s = 50, c = 'gray', label = 'Last Centroid', marker='x')\n    \n    plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:, 1],s = 200, c = 'red', label = 'Centroid', marker='*')\n    \n    centroid_history = np.append(centroid_history, kmeans.cluster_centers_, axis=0)\n    \n    plt.xlabel('Annual Income')\n    plt.ylabel('Spending Score')\n    plt.legend()\n    plt.title('Iteration:' + str(i) + ' Assignment and Move Centroid Step')\n    \n    centroid_sets = i + 1 # Adding one for initial set of centroids\n    plot_k_means_progress(centroid_history,n_clusters, centroid_sets, cluster_color)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inner Working: GIF <a id =\"19\"></a>\n\nIsn't a GIF makes a K-Means clustering visualization even more satisfying!\n\n\n![K_means_Clustering](https://raw.githubusercontent.com/satishgunjal/images/master/K_means_Clustering.gif)","metadata":{}}]}